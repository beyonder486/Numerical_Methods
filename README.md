# Numerical Methods Implementation in C++

A comprehensive collection of numerical methods implemented in C++ for solving various mathematical problems.

## Table of Contents

- [Solution of Non-Linear Equations](#solution-of-non-linear-equations)
  - [Bisection Method](#bisection-method)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)
  - [Newton-Raphson Method](#newton-raphson-method)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)
  - [Regular Falsi (False Position) Method](#regular-falsi-false-position-method)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)
  - [Secant Method](#secant-method)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)

- [Interpolation and Approximation](#interpolation-and-approximation)
  - [Newton's Forward Interpolation](#newtons-forward-interpolation)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)
  - [Newton's Backward Interpolation](#newtons-backward-interpolation)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)
  - [Newton's Divided Difference](#newtons-divided-difference)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)

- [Numerical Integration](#numerical-integration)
  - [Simpson's 1/3 Rule](#simpsons-13-rule)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)
  - [Simpson's 3/8 Rule](#simpsons-38-rule)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)

- [Curve Fitting (Regression)](#curve-fitting-regression)
  - [Linear Least Squares Regression](#linear-least-squares-regression)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)
  - [Polynomial Least Squares Regression](#polynomial-least-squares-regression)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)
  - [Exponential Least Squares Regression](#exponential-least-squares-regression)
  - [Theory](#theory)
  - [Code](#code)
  - [Input](#input)
  - [Output](#output)

---

## Solution of Non-Linear Equations

### Bisection Method

#### üìö **THEORY**
The Bisection Method is a root-finding algorithm that repeatedly bisects an interval and selects a subinterval where a root exists. It's based on the Intermediate Value Theorem:
- If f is continuous on [a,b] and f(a)¬∑f(b) < 0, then there exists at least one root in (a,b)
- The method divides the interval in half at each iteration
- Converges linearly with error reduced by half each iteration

**Algorithm:**
1. Start with interval [a,b] where f(a)¬∑f(b) < 0
2. Compute c = (a+b)/2
3. If |f(c)| < tolerance, c is the root
4. If f(a)¬∑f(c) < 0, set b = c; else set a = c
5. Repeat until convergence

---

#### üíª **CODE**
**File:** `solution of non-linear equations/bisection-Method.cpp`

---

#### üì• **INPUT**
**Format:**
```
a b tolerance maxIterations
```
**Parameters:**
- `a`: Lower bound of interval
- `b`: Upper bound of interval  
- `tolerance`: Convergence tolerance (e.g., 0.0001)
- `maxIterations`: Maximum number of iterations (e.g., 100)

**Example Input:** (`input_bisection.txt`)
```
0 5 0.0001 100
```

---

#### üì§ **OUTPUT**
**File:** `output_bisection.txt`
```
========== BISECTION METHOD RESULT ==========
Root: 0.2352941176
Iterations: 18
f(0.2352941176) = 1.234567e-05
```

---

### Newton-Raphson Method

#### üìö **THEORY**
Newton-Raphson is an iterative method that uses tangent lines to approximate roots. It has quadratic convergence near simple roots.

**Formula:**
```
x_{n+1} = x_n - f(x_n)/f'(x_n)
```

**Advantages:**
- Fast convergence (quadratic) when close to root
- Requires only one initial guess

**Disadvantages:**
- Requires derivative f'(x)
- May fail if f'(x) ‚âà 0
- May not converge if initial guess is far from root

---

#### üíª **CODE**
**File:** `solution of non-linear equations/newton-raphson-Method.cpp`

---

#### üì• **INPUT**
**Format:**
```
a b step tolerance maxIterations
```
**Parameters:**
- `a`: Start of search interval
- `b`: End of search interval
- `step`: Step size for finding initial brackets (e.g., 0.1)
- `tolerance`: Convergence tolerance (e.g., 1e-6)
- `maxIterations`: Maximum iterations per root (e.g., 1000)

**Example Input:** (`input_newton-raphson.txt`)
```
-1 2 0.1 1e-6 1000
```

---

#### üì§ **OUTPUT**
**File:** `output_newton-raphson.txt`
```
========== NEWTON-RAPHSON METHOD RESULTS ==========
Number of roots found: 3
Roots:
Root 1: -0.6180339887 (iterations: 5)
Root 2: 0.6180339887 (iterations: 6)
Root 3: 1.6180339887 (iterations: 7)

Total iterations across all roots: 18
```

---

### Regular Falsi (False Position) Method

#### üìö **THEORY**
The False Position Method is similar to the Bisection Method but uses linear interpolation instead of bisection to find the next approximation.

**Formula:**
```
c = a - f(a)¬∑(b-a)/(f(b)-f(a))
```

**Advantages:**
- Generally faster convergence than bisection
- Always converges if f is continuous on [a,b]
- Does not require derivative

**Disadvantages:**
- Can be slower than Newton-Raphson
- May converge from one side only

---

#### üíª **CODE**
**File:** `solution of non-linear equations/regularfalsi-FalsePosition-Method.cpp`

---

#### üì• **INPUT**
**Format:**
```
a b tolerance maxIterations
```
**Parameters:**
- `a`: Lower bound of interval
- `b`: Upper bound of interval
- `tolerance`: Convergence tolerance (e.g., 1e-7)
- `maxIterations`: Maximum iterations (e.g., 100)

**Example Input:** (`input_regularfalsi.txt`)
```
0 1 1e-7 100
```

---

#### üì§ **OUTPUT**
**File:** `output_regularfalsi.txt`
```
========== REGULAR FALSI (FALSE POSITION) METHOD RESULT ==========
Root: 0.2352941176
Iterations: 8
f(0.2352941176) = 1.234567e-08
Tolerance: 0.0000001
```

---

### Secant Method

#### üìö **THEORY**
The Secant Method is similar to Newton-Raphson but approximates the derivative using finite differences, eliminating the need to compute f'(x).

**Formula:**
```
x_{n+1} = x_n - f(x_n)¬∑(x_n - x_{n-1})/(f(x_n) - f(x_{n-1}))
```

**Advantages:**
- No derivative needed
- Faster than bisection
- Superlinear convergence

**Disadvantages:**
- Requires two initial guesses
- May fail if f(x_n) ‚âà f(x_{n-1})

---

#### üíª **CODE**
**File:** `solution of non-linear equations/secant-Method.cpp`

---

#### üì• **INPUT**
**Format:**
```
a b step tolerance maxIterations
```
**Parameters:**
- `a`: Start of search interval
- `b`: End of search interval
- `step`: Step size for finding brackets (e.g., 0.1)
- `tolerance`: Convergence tolerance (e.g., 1e-6)
- `maxIterations`: Maximum iterations (e.g., 1000)

**Example Input:** (`input_secant.txt`)
```
-3 3 0.1 1e-6 1000
```

---

#### üì§ **OUTPUT**
**File:** `output_secant.txt`
```
========== SECANT METHOD RESULTS ==========
Number of roots found: 3
Roots:
Root 1: -0.6180339887 (iterations: 7)
Root 2: 0.6180339887 (iterations: 8)
Root 3: 1.6180339887 (iterations: 8)

Total iterations across all roots: 23
```

---

## Interpolation and Approximation

### Newton's Forward Interpolation

#### üìö **THEORY**
Newton's Forward Interpolation uses forward differences to construct an interpolating polynomial. Best suited when interpolating near the beginning of tabulated data.

**Formula:**
```
y(x) = y‚ÇÄ + pŒîy‚ÇÄ + p(p-1)/2!¬∑Œî¬≤y‚ÇÄ + p(p-1)(p-2)/3!¬∑Œî¬≥y‚ÇÄ + ...
where p = (x - x‚ÇÄ)/h
```

**Use Cases:**
- Equally spaced data points
- Interpolation near the beginning of data
- Class interval data (grouped frequency distributions)

---

#### üíª **CODE**
**File:** `Interpolation and approximation/forwardinterpolation.cpp`

---

#### üì• **INPUT**
**Format:**
```
n
lower‚ÇÅ upper‚ÇÅ frequency‚ÇÅ
lower‚ÇÇ upper‚ÇÇ frequency‚ÇÇ
...
lower‚Çô upper‚Çô frequency‚Çô
xp
```
**Parameters:**
- `n`: Number of class intervals
- `lower upper frequency`: Class interval bounds and frequency
- `xp`: Point at which to interpolate

**Example Input:** (`input_forwardinterpolation.txt`)
```
4
10 20 5
20 30 8
30 40 12
40 50 7
25
```

---

#### üì§ **OUTPUT**
**File:** `output_forwardinterpolation.txt`
```
Class Intervals and Midpoints:
       Interval    Midpoint(x)  Frequency(y)
---------------------------------------------
     10-     20             15              5
     20-     30             25              8
     30-     40             35             12
     40-     50             45              7

Class width (h) = 10

Forward Difference Table:
...

========== RESULT ==========
y(25) = 8.500000
```

---

### Newton's Backward Interpolation

#### üìö **THEORY**
Newton's Backward Interpolation uses backward differences to construct an interpolating polynomial. Best suited when interpolating near the end of tabulated data.

**Formula:**
```
y(x) = y‚Çô + q‚àáy‚Çô + q(q+1)/2!¬∑‚àá¬≤y‚Çô + q(q+1)(q+2)/3!¬∑‚àá¬≥y‚Çô + ...
where q = (x - x‚Çô)/h
```

**Use Cases:**
- Equally spaced data points
- Interpolation near the end of data
- Class interval data with interpolation near upper bounds

---

#### üíª **CODE**
**File:** `Interpolation and approximation/backwardinterpolation.cpp`

---

#### üì• **INPUT**
**Format:**
```
n
lower‚ÇÅ upper‚ÇÅ frequency‚ÇÅ
lower‚ÇÇ upper‚ÇÇ frequency‚ÇÇ
...
lower‚Çô upper‚Çô frequency‚Çô
xp
```

**Example Input:** (`input_backwardinterpolation.txt`)
```
4
10 20 5
20 30 8
30 40 12
40 50 7
45
```

---

#### üì§ **OUTPUT**
**File:** `output_backwardinterpolation.txt`
```
Class Intervals and Midpoints:
       Interval    Midpoint(x)  Frequency(y)
---------------------------------------------
     10-     20             15              5
     20-     30             25              8
     30-     40             35             12
     40-     50             45              7

Backward Difference Table:
...

========== RESULT ==========
y(45) = 7.000000
```

---

### Newton's Divided Difference

#### üìö **THEORY**
Newton's Divided Difference method works for both equally and unequally spaced data. It constructs an interpolating polynomial using divided differences.

**Formula:**
```
y(x) = f[x‚ÇÄ] + (x-x‚ÇÄ)f[x‚ÇÄ,x‚ÇÅ] + (x-x‚ÇÄ)(x-x‚ÇÅ)f[x‚ÇÄ,x‚ÇÅ,x‚ÇÇ] + ...
```

**Advantages:**
- Works with unequally spaced data
- Easy to add new data points
- Provides error estimation

---

#### üíª **CODE**
**File:** `Interpolation and approximation/divideddifference.cpp`

---

#### üì• **INPUT**
**Format:**
```
n
x‚ÇÅ y‚ÇÅ
x‚ÇÇ y‚ÇÇ
...
x‚Çô y‚Çô
xp
```
**Parameters:**
- `n`: Number of data points
- `x y`: Data point coordinates
- `xp`: Point at which to interpolate

**Example Input:** (`input_divideddifference.txt`)
```
5
1.0 1.5
2.0 4.2
3.0 8.1
4.0 13.5
5.0 20.0
2.5
```

---

#### üì§ **OUTPUT**
**File:** `output_divideddifference.txt`
```
Data Points:
         x         f(x)
------------------------
    1.0000      1.5000
    2.0000      4.2000
    3.0000      8.1000
    4.0000     13.5000
    5.0000     20.0000

Divided Difference Table:
...

========== RESULT ==========
y(2.5) = 5.887500

========== TRUNCATION ERROR ESTIMATE ==========
Term-by-term contributions:
Term 0: 1.500000
Term 1: 1.350000 (cumulative: 2.850000)
Term 2: 1.537500 (cumulative: 4.387500)
Term 3: 1.200000 (cumulative: 5.587500)
Term 4: 0.300000 (cumulative: 5.887500)

Estimated truncation error: ‚âà 0.125000
```

---

## Numerical Integration

### Simpson's 1/3 Rule

#### üìö **THEORY**
Simpson's 1/3 Rule approximates definite integrals by fitting parabolas through consecutive triplets of points. It provides better accuracy than the Trapezoidal Rule.

**Formula:**
```
‚à´‚Çê·µá f(x)dx ‚âà (h/3)[f(x‚ÇÄ) + 4Œ£f(x·µ¢) + 2Œ£f(x‚±º) + f(x‚Çô)]
```
where i = odd indices, j = even indices (excluding endpoints)

**Requirements:**
- Number of subintervals (n) must be even
- Function must be continuous

**Error:** O(h‚Å¥)

---

#### üíª **CODE**
**File:** `Numerical Integration/simpson's1of3Rule.cpp`

---

#### üì• **INPUT**
**Format:**
```
a b n
```
**Parameters:**
- `a`: Lower limit of integration
- `b`: Upper limit of integration
- `n`: Number of subintervals (must be even)

**Example Input:** (`input_simpson1of3.txt`)
```
0 3.14159 6
```

---

#### üì§ **OUTPUT**
**File:** `output_simpson1of3.txt`
```
========== SIMPSON'S 1/3 RULE RESULT ==========
Lower limit (a): 0.000000
Upper limit (b): 3.141590
Number of subintervals (n): 6
Step size (h): 0.523598

Integral value: 2.000109
```

---

### Simpson's 3/8 Rule

#### üìö **THEORY**
Simpson's 3/8 Rule uses cubic interpolation and is similar to Simpson's 1/3 Rule but requires the number of subintervals to be a multiple of 3.

**Formula:**
```
‚à´‚Çê·µá f(x)dx ‚âà (3h/8)[f(x‚ÇÄ) + 3Œ£f(x·µ¢) + 2Œ£f(x‚±º) + f(x‚Çô)]
```
where i = non-multiples of 3, j = multiples of 3 (excluding endpoints)

**Requirements:**
- Number of subintervals (n) must be a multiple of 3
- Function must be continuous

**Error:** O(h‚Å¥)

---

#### üíª **CODE**
**File:** `Numerical Integration/Simpson's3of8Rule.cpp`

---

#### üì• **INPUT**
**Format:**
```
a b n
```
**Parameters:**
- `a`: Lower limit of integration
- `b`: Upper limit of integration
- `n`: Number of subintervals (must be multiple of 3)

**Example Input:** (`input_simpson3of8.txt`)
```
0 3.14159 6
```

---

#### üì§ **OUTPUT**
**File:** `output_simpson3of8.txt`
```
========== SIMPSON'S 3/8 RULE RESULT ==========
Lower limit (a): 0.000000
Upper limit (b): 3.141590
Number of subintervals (n): 6
Step size (h): 0.523598

Integral value: 2.000459
```

---

## Curve Fitting (Regression)

### Linear Least Squares Regression

#### üìö **THEORY**
Linear Least Squares finds the best-fit straight line y = ax + b that minimizes the sum of squared residuals.

**Normal Equations:**
```
a = (nŒ£xy - Œ£xŒ£y)/(nŒ£x¬≤ - (Œ£x)¬≤)
b = (Œ£y - aŒ£x)/n
```

**Goodness of Fit:**
- R¬≤ (Coefficient of Determination): Measures how well the line fits the data (0 to 1)
- Correlation Coefficient (r): Measures linear relationship strength (-1 to 1)

---

#### üíª **CODE**
**File:** `Curve fitting(Regression)/lsr-Linear.cpp`

---

#### üì• **INPUT**
**Format:**
```
n
x‚ÇÅ y‚ÇÅ
x‚ÇÇ y‚ÇÇ
...
x‚Çô y‚Çô
```
**Parameters:**
- `n`: Number of data points
- `x y`: Data point coordinates

**Example Input:** (`input_lsr_Linear.txt`)
```
5
1.0 2.5
2.0 4.2
3.0 5.9
4.0 8.1
5.0 9.8
```

---

#### üì§ **OUTPUT**
**File:** `output_lsr_Linear.txt`
```
Data Points:
         x         y
--------------------
    1.0000    2.5000
    2.0000    4.2000
    3.0000    5.9000
    4.0000    8.1000
    5.0000    9.8000

========== CALCULATIONS ==========
Œ£x   = 15
Œ£y   = 30.5
Œ£x¬≤  = 55
Œ£xy  = 103.5
n    = 5

========== LEAST SQUARES LINEAR REGRESSION ==========
Best fit line: y = 1.8700x + 0.7400

========== GOODNESS OF FIT ==========
R¬≤ (Coefficient of Determination): 0.9945
Correlation Coefficient (r):       0.9972
Interpretation: Excellent fit (R¬≤ > 0.9)
```

---

### Polynomial Least Squares Regression

#### üìö **THEORY**
Polynomial Least Squares fits a polynomial of degree m: y = a‚ÇÄ + a‚ÇÅx + a‚ÇÇx¬≤ + ... + a‚Çòx·µê

The method solves the system of normal equations using Gauss-Jordan elimination to find coefficients that minimize the sum of squared residuals.

**Normal Equations:** (m+1) √ó (m+1) system
```
[Œ£x‚Å∞  Œ£x¬π  ... Œ£x·µê  ] [a‚ÇÄ]   [Œ£y¬∑x‚Å∞]
[Œ£x¬π  Œ£x¬≤  ... Œ£x·µê‚Å∫¬π] [a‚ÇÅ] = [Œ£y¬∑x¬π]
[  ‚ãÆ    ‚ãÆ   ‚ã±   ‚ãÆ   ] [‚ãÆ ]   [  ‚ãÆ  ]
[Œ£x·µê  Œ£x·µê‚Å∫¬π ... Œ£x¬≤·µê] [a‚Çò]   [Œ£y¬∑x·µê]
```

---

#### üíª **CODE**
**File:** `Curve fitting(Regression)/lsr_polynomial.cpp`

---

#### üì• **INPUT**
**Format:**
```
n degree
x‚ÇÅ y‚ÇÅ
x‚ÇÇ y‚ÇÇ
...
x‚Çô y‚Çô
```
**Parameters:**
- `n`: Number of data points
- `degree`: Degree of polynomial (e.g., 2 for quadratic)
- `x y`: Data point coordinates

**Example Input:** (`input_lsr_polynomial.txt`)
```
5 2
1.0 2.5
2.0 5.4
3.0 10.1
4.0 15.9
5.0 23.5
```

---

#### üì§ **OUTPUT**
**File:** `output_lsr_polynomial.txt`
```
========== POLYNOMIAL LEAST SQUARES REGRESSION ==========
Degree: 2
Number of data points: 5

Best fit polynomial equation:
y = 0.371429 + 0.571429x + 0.928571x^2
```

---

### Exponential Least Squares Regression

#### üìö **THEORY**
Exponential Least Squares fits data to the model: y = a¬∑e·µáÀ£

By taking logarithms: ln(y) = ln(a) + bx, which is linear in ln(y).

**Steps:**
1. Transform: Y = ln(y)
2. Apply linear regression on (x, Y)
3. Solve for b and ln(a)
4. Convert back: a = eÀ°‚Åø‚ÅΩ·µÉ‚Åæ

**Requirements:**
- All y values must be positive (cannot take log of negative numbers)

---

#### üíª **CODE**
**File:** `Curve fitting(Regression)/lsr_transcedental.cpp`

---

#### üì• **INPUT**
**Format:**
```
n
x‚ÇÅ y‚ÇÅ
x‚ÇÇ y‚ÇÇ
...
x‚Çô y‚Çô
```
**Parameters:**
- `n`: Number of data points (must be ‚â• 2)
- `x y`: Data points (all y values must be positive)

**Example Input:** (`input_lsr_transcedental.txt`)
```
5
1.0 2.5
2.0 7.2
3.0 18.5
4.0 49.0
5.0 132.5
```

---

#### üì§ **OUTPUT**
**File:** `output_lsr_transcedental.txt`
```
========== EXPONENTIAL LEAST SQUARES REGRESSION ==========
Number of data points: 5

Best fit exponential equation:
y = 1.025643 * e^(1.012345x)
```

---

## Usage Instructions

### Compilation
All programs can be compiled using g++:
```bash
g++ filename.cpp -o outputname
```

Example:
```bash
g++ "solution of non-linear equations/bisection-Method.cpp" -o bisection
```

### Execution
Programs read from `input_*.txt` and write to `output_*.txt`:
```bash
./bisection
```

### Input Files
Each program has a corresponding input file:
- `input_bisection.txt`
- `input_newton-raphson.txt`
- `input_regularfalsi.txt`
- `input_secant.txt`
- `input_forwardinterpolation.txt`
- `input_backwardinterpolation.txt`
- `input_divideddifference.txt`
- `input_simpson1of3.txt`
- `input_simpson3of8.txt`
- `input_lsr_Linear.txt`
- `input_lsr_polynomial.txt`
- `input_lsr_transcedental.txt`

### Output Files
Results are written to corresponding output files:
- `output_bisection.txt`
- `output_newton-raphson.txt`
- etc.

---

## Error Handling

All programs include comprehensive error checking:
- **Division by zero** protection
- **Singular matrix** detection
- **Invalid input** validation
- **Convergence** monitoring
- **Iteration limits** to prevent infinite loops

---
